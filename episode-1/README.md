#Episode 1: Micrograd

In this folder we have everything from our first walkthrough of Kaparthy's micrograd build. In it we have the full build of micrograd:
  - **engine.py**. This file contains the Value class which is like a scalar valued tensor in PyTorch. In this we followed Andrej's build entirely, with the addition (I think) of a few activation functions. Basically, we implement backpropogation that travels through a directed acyclic graph (DAG) making up the component pieces of a neuron. Basically, breaking down the complexity of a neuron (matrix multiplications and activations) into individual operations over pairs of values. 
  - **engine.py**. This file contains the basics to build a neural net: Neuron, Layer, Multi-Layer-Perceptron (MLP). Since some of the experiments below required slightly different setups the MLP supports specialized activations for the hidden layers as well as a separate activation for outter layer.
      - ex: MLP(3,[18, 8, 1], inner_activation='tanh', final_activation='sigmoid') will produce a model with 3 input parameters, 2 hidden layers (of 18 and 6 neurons resp.) with tanh activations and a final output with sigmoid activation. 
   

  The folder experiments contains various experiments that we performed using micrograd. A few of these were suggested/performed by Karpathy though we tried to do these mostly without looking at specifics so I cannot say how closely they follow Andrej's nor how wonderful they are. We did, however, attempt to produce visuals along the way to help ourselves understand the MLP and what the neurons tend to focus on. Anyways, a bit of what we tried:
  - **Linear regression**. Key takeaway (and the reason we threw in the specialized activation calls) was that tanh as an outter activation function does not predict linear trends in data very well! (who would have guessed!). Anyways, it was a nice confirmation that the model works!
  - **XOR**. So, having done a few more experiments since XOR I realize that I should have used a margin based classifier on this example... Alas, I did not. And it shows. That said, the model did very well at classifying and, although I would like to redo this experiment with said margin based classifier, we got a chance to play with matplotlib a bit to produce some gifs of our experiments.
      - ![xor decision surface](/experiments/xor/xor_decision_surface_8nhl_4nhl_1200_epochs.gif)
  -  **2D-partition (Moons dataset)**. This is the one that (thanks to ChatGPT) I realized that I should be using a margin based classifier. At the time of this writing I have realized that Kaparthy does in fact use a hinge loss so I should have just looked there at first!
  -  **MNIST**. This one broke the micrograd... or at least my patience for the micrograd. I thought it would be interesting to take the MNIST handwritten digit data and try to train a micrograd model to distinguish between 0 and 1. I downloaded, sorted and batched the data. Got a model set up with 2 hidden layers of 8 neurons each and started the experiment. It ran, and ran, and ran. And ran. We halted, rebatched (2 examples per batch) and ran. It ran and ran and ran, but this time it produced 6 outputs in the time I gave it, losses were decreasing but it was time to switch to Pytorch! 
